<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>seldonian.optimizers.gradient_descent.gradient_descent_adam &#8212; Seldonian Engine pre-release documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=bc8d421b" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=6a9260ee"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="seldonian.optimizers.gradient_descent.setup_gradients" href="seldonian.optimizers.gradient_descent.setup_gradients.html" />
    <link rel="prev" title="seldonian.optimizers.gradient_descent" href="seldonian.optimizers.gradient_descent.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="seldonian-optimizers-gradient-descent-gradient-descent-adam">
<h1>seldonian.optimizers.gradient_descent.gradient_descent_adam<a class="headerlink" href="#seldonian-optimizers-gradient-descent-gradient-descent-adam" title="Permalink to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="seldonian.optimizers.gradient_descent.gradient_descent_adam">
<span class="sig-name descname"><span class="pre">gradient_descent_adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">primary_objective</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_constraints</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper_bounds_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_calculator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_batches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_theta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_lamb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_velocity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_rmsprop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_library</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'autograd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#seldonian.optimizers.gradient_descent.gradient_descent_adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements simultaneous gradient descent/ascent using
the Adam optimizer on a Lagrangian:
L(theta,lambda) = f(theta) + lambda*g(theta),
where f is the primary objective, lambda is a vector of
Lagrange multipliers, and g is a vector of the
upper bound functions. Gradient descent is done for theta
and gradient ascent is done for lambda to find the saddle
points of L. Being part of candidate selection,
it is important that this function always returns a solution.
The safety test determines if No Solution Found.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>primary_objective</strong> (<em>function</em><em> or </em><em>class method</em>) – The objective function that would
be solely optimized in the absence of behavioral constraints,
i.e., the loss function</p></li>
<li><p><strong>n_constraints</strong> – The number of constraints</p></li>
<li><p><strong>upper_bounds_function</strong> (<em>function</em><em> or </em><em>class method</em>) – The function that calculates
the upper bounds on the constraints</p></li>
<li><p><strong>theta_init</strong> (<em>float</em>) – Initial model weights</p></li>
<li><p><strong>lambda_init</strong> – Initial values for Lagrange multiplier terms</p></li>
<li><p><strong>alpha_theta</strong> (<em>float</em>) – Initial learning rate for theta</p></li>
<li><p><strong>alpha_lamb</strong> (<em>float</em>) – Initial learning rate for lambda</p></li>
<li><p><strong>beta_velocity</strong> (<em>float</em>) – Exponential decay rate for velocity term</p></li>
<li><p><strong>beta_rmsprop</strong> (<em>float</em>) – Exponential decay rate for rmsprop term</p></li>
<li><p><strong>num_iters</strong> (<em>int</em>) – The number of iterations of gradient descent to run</p></li>
<li><p><strong>gradient_library</strong> (<em>str</em><em>, </em><em>defaults to &quot;autograd&quot;</em>) – The name of the library to use for computing
automatic gradients.</p></li>
<li><p><strong>verbose</strong> – Boolean flag to control verbosity</p></li>
<li><p><strong>debug</strong> – Boolean flag to print out info useful for debugging</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>solution, a dictionary containing the solution and metadata
about the gradient descent run</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Seldonian Engine</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="seldonian.html">seldonian</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="seldonian.RL.html">seldonian.RL</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.candidate_selection.html">seldonian.candidate_selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.dataset.html">seldonian.dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.hyperparam_search.html">seldonian.hyperparam_search</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.models.html">seldonian.models</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="seldonian.optimizers.html">seldonian.optimizers</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="seldonian.optimizers.gradient_descent.html">seldonian.optimizers.gradient_descent</a><ul class="current">
<li class="toctree-l5 current"><a class="current reference internal" href="#">seldonian.optimizers.gradient_descent.gradient_descent_adam</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#seldonian.optimizers.gradient_descent.gradient_descent_adam"><code class="docutils literal notranslate"><span class="pre">gradient_descent_adam()</span></code></a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="seldonian.optimizers.gradient_descent.setup_gradients.html">seldonian.optimizers.gradient_descent.setup_gradients</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.parse_tree.html">seldonian.parse_tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.safety_test.html">seldonian.safety_test</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.seldonian_algorithm.html">seldonian.seldonian_algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.spec.html">seldonian.spec</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.utils.html">seldonian.utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="seldonian.warnings.html">seldonian.warnings</a></li>
</ul>
</li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="../api.html">API</a><ul>
  <li><a href="seldonian.html">seldonian</a><ul>
  <li><a href="seldonian.optimizers.html">seldonian.optimizers</a><ul>
  <li><a href="seldonian.optimizers.gradient_descent.html">seldonian.optimizers.gradient_descent</a><ul>
      <li>Previous: <a href="seldonian.optimizers.gradient_descent.html" title="previous chapter">seldonian.optimizers.gradient_descent</a></li>
      <li>Next: <a href="seldonian.optimizers.gradient_descent.setup_gradients.html" title="next chapter">seldonian.optimizers.gradient_descent.setup_gradients</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, University of Massachusetts.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../_sources/_autosummary/seldonian.optimizers.gradient_descent.gradient_descent_adam.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>